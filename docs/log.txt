We have decided to write a log file to document the evolution of the project.

June 19:
    Started creating the work plan for the project.
    While thinking about the hyper parameters noticed that we also need to handle cases where two similar
    sounds are consecutive because that can also create homophonic similarity where in one case the sound
    appears once and in the other it appears twice. Example (brit): "No eyed dear" vs. "No idea"

June 24:
    Created a psuedo-code algorithm for the phonetic part of the project.

July 4:
    Created first version of training. Decided to leave the hyper-parameters for later since the 
    first priority is having a working pipeling. Created only the structure for the the hyper-parameters.

July 8:
    First successful synthesis of non-filtered sentence matches. Noticed that since the sentences are not
    punctuated, rather than filtering the invalid ones out, we need to check which ones can be punctuated
    correctly.
    
July 18:
    Implemented a hyper-parameter for replacing groups of phonemes. Discovered that current synthesis
    algorithm requires too much memory. Will need to redo it.

July 19:
    Tried to switch the recursive function in the synthesis algorithmwith a FIFO queue. It didn't help, and because 
    of added traverses of the tree may have added a little to complexity. After delving into the problem in greater 
    depth discovered that the problem is that when applying the hyper-parameters just a little, the number of matches 
    rises to enormous numbers: for the sentence "HE DIDN'T CEDER TREE" there were above 1M matches.
    Decided to try dumping into the output file after every sentence. The first 19 sentences produced over 147MB.
    The 20th sentence, "DOLPHINS DON'T SPEAK WELSH ONLY WALES", on which the memory kept overflowing, still took 
    an extremely long time to compute and crashed on a Memory Error.
    Decided to switch to a LIFO queue so that the queue will not fill exponentially before emptying. The trick
    worked, yielding for above sentence more than 11M matches. The program still crashed with a Memory Error, but
    the information achieved by the test was sufficient: tweaking this specific type of hyper-parameter has a
    devastating effect in the current pipeline arrangement with the CMU phonetic dictionary.
    Will need to think of next step. Probably will have to either filter from the CMU dictionary words that are not
    proper English or Proper Names, or will have to check matches for correctness already at this stage, or might
    have to find a better solution for the necessity which required the hyper-parameters in the first place.

July 24:
    Decided to install NLTK so as to try to use its resources for various aspects of the project.
    To install NLTK in Windows type in the command line in the Python directory (The EXE they provide doesn't work):
        python -n pip install nltk
    In addition to installing NLTK there are various extensions needed (not sure all will be necessary in final project but 
    advise to add all in any case). Here is the list of commands to use in the Python interpreter (will add more when
    necessary):
        nltk.download('punkt')
        nltk.download('averaged_perceptron_tagger')
        nltk.download('wordnet')
        nltk.download('words')
    After installing NLTK tried to create new hyper-parameter filter using the "synsets" function. It failed on words like
    "they", "with" and "don't". Adding nltk.corpus.words helped for "they" and "with" but not "don't".

July 25:
    Decided to add a list of contractions (words like "don't") manually. Now no words from the current test file are a
    problem but it doesn't prove that no words are in fact a problem. Will consider dealing with missing words via
    web service. 
    Used the current version to see implications of word filter. Without the replace hyper-parameter shortened the list
    by what seems to be a constant factor. With the replace hyper-parameter gave only about 6K matches for "HE DIDN'T CEDER 
    TREE" but still has more than 9M matches (and counting - terminated it in the middle) for "DOLPHINS DON'T SPEAK WELSH ONLY 
    WALES".
    Will consider using the replace hyper-parameter only where a pun was not found without it. 
    It is possible the current configuration of the system is only good for very short sentences. In that case must consider
    creating a sub-system for shortening the sentence given to only the words that need to be checked for a pun.

July 26:
    Decided that since a pun usually is in only a small and consequent part of a sentence we can add a general threshold
    parameter that will make the system check only up to N words at a time leaving the rest of the sentence without changes.
    This necessitated to rearrange the rules so that they are not only for training but for all the stages.
    
July 29:
    Applied said threshold to analysis and synthesis. Had major success: replace + WordNet + threshold 4 produced less 
    than 49K matches for "DOLPHINS DON'T SPEAK WELSH ONLY WALES".

July 31:
    Started considering the "NO IDEA" == "NO EYED DEER" problem. The obvious place for it is in the synthesis stage.
    Basically, the necessary changes are in the following cases:
        Assuming a phoneme A ending a word followed by phonemes B C:
            1. A!=B - The next word begins with B C (Regular).
            2. A!=B - The next word begins with A B C (Sticky).
            3. A==B - The next word begins with B C (Regular).
            4. A==B - The next word begins with C (Sticky).
        Assuming consecutive phonemes A B B C:
            5. Process phonemes as A B B C (Regular).
            6. Process phonemes as A B C (Sticky).
    While trying to find correct terminology for such a situation found this interesting Wikipedia page:
        https://en.wikipedia.org/wiki/Mondegreen
    Although Mondegreen is a related term it is too general and more relevant for the general intention of the project.
    For now will probably stick to the expression "Sticky Words".

August 5:
    Have applied the cases of "Sticky Words". Cases 2 and 4 will need more work because they cause a Memory Error.
    
August 11:
    Tried using NLTK for cleaning results, as an alternative or in addition to the LOGON online parser. The results were not
    that usable - weaker than the LOGON parser. Maybe we need to define a grammar for it to work. Something that can be 
    investigated if required.
    This required installing numpy (pip3 install numpy) and installing certificate 
    (/Applications/Python 3.7$ ./Install\ Certificates.command)
    
    Tried using Google search and filter sentences with no results - a strong and useful filter.
    This required installing Google package (pip3 install google).
    After a few runs Google blocked the requests due to their anti-bot policy. To overcome this we may signup to 
    developers.google.com and use their api (might be a small fee).
    One option was using Bing, but they have a limit for transactions as well (1000 transactions per month) which does not
    work for us.

August 14:
    After having fixed case 5 of "Sticky Words", Reconsidered cases 2 and 4. It seems that not all phonemes are "Sticky".
    For instance, the phoneme AW won't stick to another AW because being a diphtong, it starts with AH and ends with UW so
    twice AW (e.g. "how owl" sounding HH AW AW L) won't stick. It seems that cases 2 and 4 must be simplified to specific 
    rules for the phonemes which can stick. The problem is that there are too many cases like that to conveniently write 
    rules for all of them. Might need cleverly written rules to easily cover many cases.
