We have decided to write a log file to document the evolution of the project.

June 19:
    Started creating the work plan for the project.
    While thinking about the hyper parameters noticed that we also need to handle cases where two similar
    sounds are consecutive because that can also create homophonic similarity where in one case the sound
    appears once and in the other it appears twice. Example (brit): "No eyed dear" vs. "No idea"

June 24:
    Created a psuedo-code algorithm for the phonetic part of the project.

July 4:
    Created first version of training. Decided to leave the hyper-parameters for later since the 
    first priority is having a working pipeling. Created only the structure for the the hyper-parameters.

July 8:
    First successful synthesis of non-filtered sentence matches. Noticed that since the sentences are not
    punctuated, rather than filtering the invalid ones out, we need to check which ones can be punctuated
    correctly.
    
July 18:
    Implemented a hyper-parameter for replacing groups of phonemes. Discovered that current synthesis
    algorithm requires too much memory. Will need to redo it.

July 19:
    Tried to switch the recursive function in the synthesis algorithmwith a FIFO queue. It didn't help, and because 
    of added traverses of the tree may have added a little to complexity. After delving into the problem in greater 
    depth discovered that the problem is that when applying the hyper-parameters just a little, the number of matches 
    rises to enormous numbers: for the sentence "HE DIDN'T CEDER TREE" there were above 1M matches.
    Decided to try dumping into the output file after every sentence. The first 19 sentences produced over 147MB.
    The 20th sentence, "DOLPHINS DON'T SPEAK WELSH ONLY WALES", on which the memory kept overflowing, still took 
    an extremely long time to compute and crashed on a Memory Error.
    Decided to switch to a LIFO queue so that the queue will not fill exponentially before emptying. The trick
    worked, yielding for above sentence more than 11M matches. The program still crashed with a Memory Error, but
    the information achieved by the test was sufficient: tweaking this specific type of hyper-parameter has a
    devastating effect in the current pipeline arrangement with the CMU phonetic dictionary.
    Will need to think of next step. Probably will have to either filter from the CMU dictionary words that are not
    proper English or Proper Names, or will have to check matches for correctness already at this stage, or might
    have to find a better solution for the necessity which required the hyper-parameters in the first place.

July 24:
    Decided to install NLTK so as to try to use its resources for various aspects of the project.
    To install NLTK in Windows type in the command line in the Python directory (The EXE they provide doesn't work):
        python -n pip install nltk
    In addition to installing NLTK there are various extensions needed (not sure all will be necessary in final project but 
    advise to add all in any case). Here is the list of commands to use in the Python interpreter (will add more when
    necessary):
        nltk.download('punkt')
        nltk.download('averaged_perceptron_tagger')
        nltk.download('wordnet')
        nltk.download('words')
    After installing NLTK tried to create new hyper-parameter filter using the "synsets" function. It failed on words like
    "they", "with" and "don't". Adding nltk.corpus.words helped for "they" and "with" but not "don't".

July 25:
    Decided to add a list of contractions (words like "don't") manually. Now no words from the current test file are a
    problem but it doesn't prove that no words are in fact a problem. Will consider dealing with missing words via
    web service. 
    Used the current version to see implications of word filter. Without the replace hyper-parameter shortened the list
    by what seems to be a constant factor. With the replace hyper-parameter gave only about 6K matches for "HE DIDN'T CEDER 
    TREE" but still has more than 9M matches (and counting - I stopped it in the middle) for "DOLPHINS DON'T SPEAK WELSH ONLY 
    WALES".
    Will consider using the replace hyper-parameter only where a pun was not found without it. 
    It is possible the current configuration of the system is only good for very short sentences. In that case must consider
    creating a sub-system for shortening the sentence given to only the words that need to be checked for a pun.
